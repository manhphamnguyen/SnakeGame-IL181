# SnakeGame-IL181

This project seeks to implement the snake game using Q-Learning. 

# Instruction:

Install gym on your computer, and use this folder gym_snake to run the environment. The available gym_snake module from OpenAI gym runs too slow and has a rendering problem so I decided to create this different version that runs much faster without any problems. To change the size of the snake game, go to gym_snake/envs/snake_env and change the self.width and self.height parameters (default is (15,15)). You can also change the reward values, dafult is -1 reward each step, 100 if the snake eat the dot, -100 if it dies, and 10,000,000,000 if it wins the game.

File snake_bfs.py tries to solve the problem using Breadth First Search and performs relatively well, usually eating quite a few apples before dying, with a total reward of +300 on average (the file also plot out its reward each game). Its biggest problem is when the snake gets long enough, it tends to trap itself and lose the game, which is typically unavoidable at some point since -1 reward is applied each move (otherwise, we can create a pattern that goes through all square over and over, which guarantees winning the game, but the accumulated -1 rewards would be too high). 

As for Q-Learning in the snake_QLearning.py file, while it is theorectically possible to find an optimal strategy for this game using reinforcement learning, the possible number of states in the game is exponentially high as the snake can get to hundreds of tiles long, with countless positional variations for the snake. Thus, it is impractical to use this default state for Q-Learning. For my first version I considered the state as a vector of 6 variables (a,b,c,d,w,h) with the first 4 variables are booleans to see whether the immediate left, right, up, down of the snake head has an obstacle, encouraging it to avoid ones that have obstacles (wall or itself). The last two shows the width and height difference between the snake head and the dot, incentivizing the snake to approach the dot by minimizing these values. However, this method yields 2^4.15.15=3600 states (depending on the board size), which is still a high number, leading to slow convergence. In this second version I also use 6 variables (a,b,c,d,x,y), but now the last 2 variables show the relative position of the snake head compared to the apple, instead of exact values like before. x = -1,0,1 when the snake is on the right, same, left of the apple on the vertical axis. y = -1,0,1 when the snake is on the bottom, same, top of the apple on the horizontal axis. There are 2^4.3.3 = 144 states in total, much smaller than the last version, so the algorithm is expected to converge much faster.

Hence, this Breadth First Search strategy is the threshold that I want the Q-learning algorithm to achieve. If the algorithm reach reward value near or beyond BFS strategy, Q Learning would then can be a useful tool for the snake game. For the Q-Learning python file, I play 200 games during the training phase, and update the Q-table using each move of these games. The training reward is also modified to that it nudges the snake to converge towards the apple by rewarding the snake when it is in the same axis as the apple, and punishes it when it leaves this axis. The game is expected to yield better rewards over time.
